{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# cyberagent/open-calm-7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu6onswoecPe"
      },
      "source": [
        "* https://huggingface.co/cyberagent/open-calm-7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/data-science-wiki/blob/main/nlp/text_generation(japanese)/cyberagent_open_calm_7b.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vdnw6C1vegWi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install safetensors\n",
        "# \u4ee5\u4e0b\u3092\u5b9f\u884c\u5f8c\u30ea\u30b9\u30bf\u30fc\u30c8\n",
        "!pip install torch==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0053d567c6484fc5b36a7847d9e147d8",
            "7977ebe46fc74c65af300e37789575bf",
            "4c15e4776f584f98bb577758abb99af1",
            "e40f21ee0e42482fb1b561e3aaf008db",
            "c00ab3415e1840bda7f07b4b943e5f01",
            "e5e0e129396541cb9eac9b715d0b4f21",
            "4b02e4a979514db0b74efe74569c9fbe",
            "d062df8c38b94c24ac23778868935f42",
            "0508031be03149b894a93b1b68bd6de0",
            "d6481eefb8f14f4683b3d1098d94e435",
            "59efa5fb1a994607b22c6a798135b5a0"
          ]
        },
        "id": "9nkEOMIWeSZe",
        "outputId": "88b2c334-1248-4d64-a631-7ebbc304d144"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0053d567c6484fc5b36a7847d9e147d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/open-calm-7b\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2yPGFYu93KWE"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "\u6a5f\u68b0\u5b66\u7fd2\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtS4Dmq23FKg",
        "outputId": "b172be44-595e-4a52-b27c-c6a4f92af984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u6a5f\u68b0\u5b66\u7fd2\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\n",
            "\u6a5f\u68b0\u5b66\u7fd2\u306b\u3064\u3044\u3066\u3001\u4eca\u3001\u77e5\u308a\u305f\u3044\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\n",
            "\u3042\u308b\u30c7\u30fc\u30bf\u304c\u3001\u4eca\u3001\u3042\u308b\u30c7\u30fc\u30bf\u3088\u308a\u3082\u3001\u591a\u304f\u306e\u3053\u3068\u3092\u6559\u3048\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u304c\u3001\u3042\u308b\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u3001\u4eca\u3042\u308b\u30c7\u30fc\u30bf\u3088\u308a\u3082\u3001\u591a\u304f\u306e\u3053\u3068\u3092\u6559\u3048\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u304c\u3001\u3042\u308b\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u3001\u4eca\u3042\u308b\u30c7\u30fc\u30bf\u3088\u308a\u3082\u3001\u591a\u304f\u306e\u3053\u3068\u3092\u6559\u3048\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u304c\u3001\u3042\u308b\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(model.device)\n",
        "with torch.no_grad():\n",
        "    tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    \n",
        "output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uNr2blcIerC5"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "1+3=4\n",
        "7+1=8\n",
        "3+8=11\n",
        "7+8=15\n",
        "1+8=\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IZzo-ldelIH",
        "outputId": "36c21c8e-ae0d-4607-d61e-95ad22d685a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1+3=4\n",
            "7+1=8\n",
            "3+8=11\n",
            "7+8=15\n",
            "1+8=\n",
            "3+4=6\n",
            "1+2=3\n",
            "7+0=7\n",
            "2+3=4\n",
            "4+0=0\n",
            "4-3=0\n",
            "1+0=0\n",
            "2+0=0\n",
            "3+0=3\n",
            "0+0=0\n",
            "4-0=\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(model.device)\n",
        "with torch.no_grad():\n",
        "    tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    \n",
        "output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNNA0DkVRBMOfr8WtoqZ5Fo",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.18 ('.venv': poetry)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "17a011378fed683b21aba93e5dd7c0cb7beefc09c5af72c6425b40c713e260dc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}