{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-BERT\n",
    "* https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/data-science-wiki/blob/main/nlp/text_embedding(japanese)/sentencebert.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install fugashi\n",
    "!pip install ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604cbbfb74c941c6a3eb86cb352336a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/258k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21297474e25b47f1ad31390a3c16b8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5d1b0524424ea1a1d06b42efae3e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:458\u001b[0m, in \u001b[0;36mMecabTokenizer.__init__\u001b[0;34m(self, do_lower_case, never_split, normalize_text, mecab_dic, mecab_option)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfugashi\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fugashi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# return torch.stack(all_embeddings).numpy()\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(all_embeddings)\n\u001b[0;32m---> 38\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceBertJapanese\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msonoisa/sentence-bert-base-ja-mean-tokens-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36mSentenceBertJapanese.__init__\u001b[0;34m(self, model_name_or_path, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name_or_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertJapaneseTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1841\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1839\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2004\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2004\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2007\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2008\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2009\u001b[0m     )\n",
      "File \u001b[0;32m~/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:211\u001b[0m, in \u001b[0;36mBertJapaneseTokenizer.__init__\u001b[0;34m(self, vocab_file, spm_file, do_lower_case, do_word_tokenize, do_subword_tokenize, word_tokenizer_type, subword_tokenizer_type, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, mecab_kwargs, sudachi_kwargs, jumanpp_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_tokenizer \u001b[38;5;241m=\u001b[39m BasicTokenizer(\n\u001b[1;32m    208\u001b[0m         do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case, never_split\u001b[38;5;241m=\u001b[39mnever_split, tokenize_chinese_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word_tokenizer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmecab\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMecabTokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnever_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmecab_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word_tokenizer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msudachi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_tokenizer \u001b[38;5;241m=\u001b[39m SudachiTokenizer(\n\u001b[1;32m    216\u001b[0m         do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case, never_split\u001b[38;5;241m=\u001b[39mnever_split, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(sudachi_kwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[1;32m    217\u001b[0m     )\n",
      "File \u001b[0;32m~/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/transformers/models/bert_japanese/tokenization_bert_japanese.py:460\u001b[0m, in \u001b[0;36mMecabTokenizer.__init__\u001b[0;34m(self, do_lower_case, never_split, normalize_text, mecab_dic, mecab_option)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfugashi\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install fugashi to use MecabTokenizer. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://pypi.org/project/fugashi/ for installation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    463\u001b[0m     )\n\u001b[1;32m    465\u001b[0m mecab_option \u001b[38;5;241m=\u001b[39m mecab_option \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mecab_dic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: You need to install fugashi to use MecabTokenizer. See https://pypi.org/project/fugashi/ for installation."
     ]
    }
   ],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    '機械学習の勉強をする',\n",
    "    'あいうえお',\n",
    "    '量子コンピュータは量子力学の原理を応用したコンピュータ'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(text, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eea3600efe48a789c030f92f327850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25160a7a4f84416297149a6080ff86ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[-8.4269e-01,  8.6399e-01,  7.0584e-01, -1.8740e-02,  9.8217e-01,\n",
      "         -8.3913e-01,  1.5679e+00, -5.5971e-01,  8.4783e-03,  1.3139e+00,\n",
      "          4.2518e-01,  6.0655e-01, -9.0300e-01, -7.1108e-01, -1.4093e-01,\n",
      "         -1.6973e-02,  1.7206e-01,  7.2887e-01,  1.2516e-01, -1.5166e-01,\n",
      "          2.3493e-01,  8.2472e-02,  4.3417e-01, -5.9426e-01, -7.9133e-01,\n",
      "         -2.2210e-01, -8.2877e-01, -3.1285e-01,  4.7588e-01,  8.1095e-01,\n",
      "          2.9929e-01, -8.2731e-01,  5.8782e-01,  9.9275e-01, -4.7253e-01,\n",
      "          2.8159e-01, -2.5384e-02,  9.9497e-01,  5.4013e-01, -1.3688e+00,\n",
      "         -8.8183e-01,  1.7397e+00, -1.3928e+00,  4.1439e-01,  7.4225e-01,\n",
      "          7.2051e-01,  4.0400e-01,  2.8564e-01,  2.7980e-01, -1.1960e+00,\n",
      "          8.6408e-01,  4.5193e-01,  2.0175e-01, -1.0968e-01, -6.5506e-01,\n",
      "          4.4915e-01,  2.6203e-01,  2.0201e-01,  1.1133e+00,  1.2912e+00,\n",
      "         -4.8711e-01, -9.0625e-01,  4.5924e-01, -2.8673e-01, -1.2116e-02,\n",
      "         -1.2662e-01, -9.1644e-01,  1.2798e-01, -5.7775e-01, -1.5596e+00,\n",
      "         -4.2997e-01, -1.7408e-01,  8.9599e-02,  4.4305e-01, -6.6751e-01,\n",
      "         -4.7519e-01, -8.7155e-01,  1.1153e-01, -1.0401e+00,  1.2543e+00,\n",
      "          6.0747e-01,  1.2489e+00,  4.3303e-01,  8.1317e-01, -2.2821e-01,\n",
      "         -1.1861e+00, -2.4609e-01, -1.1891e-01, -7.6232e-01,  7.8048e-01,\n",
      "          1.0022e+00, -6.8586e-01, -6.6985e-01,  2.5666e-01,  6.3097e-01,\n",
      "          3.3400e-01, -2.6127e-01,  1.7155e+00, -3.1883e-01,  1.4097e+00,\n",
      "         -5.8327e-01,  1.2633e+00,  1.3777e+00,  5.9134e-01, -6.1623e-01,\n",
      "         -1.6201e-01, -9.9172e-02,  1.4133e+00, -5.9081e-01,  1.2025e-01,\n",
      "          1.1985e-01, -1.5802e-01, -7.5691e-01,  1.0879e-01,  9.1617e-03,\n",
      "          1.1659e-01,  2.7882e-01, -5.5909e-01, -3.3861e-01, -1.0604e-01,\n",
      "         -1.3633e+00, -1.1908e+00, -8.6338e-01,  3.4333e-01, -7.2295e-02,\n",
      "         -3.8789e-01,  1.2079e-01, -3.4418e-01,  6.2796e-01,  6.2409e-01,\n",
      "         -2.6009e-02, -8.7195e-01,  1.1385e+00, -1.3959e-01, -6.9173e-01,\n",
      "          2.8655e-02, -5.2091e-01, -5.8811e-01,  1.1571e+00,  4.4539e-01,\n",
      "         -5.1665e-01, -3.0528e-01, -3.4898e-01,  7.8883e-01, -2.2844e-03,\n",
      "         -3.0319e-01, -1.6170e-01,  1.2143e+00,  1.1749e+00,  7.0888e-01,\n",
      "         -9.0840e-02, -1.0426e+00,  5.6187e-01,  9.2597e-01, -5.0473e-01,\n",
      "         -4.0295e-01,  5.8174e-01, -1.7899e-01,  1.0478e-01,  1.8179e-01,\n",
      "          3.5594e-01,  3.5531e-01, -1.6800e-01,  8.3106e-01,  3.7138e-01,\n",
      "         -5.4312e-01,  5.1840e-01,  8.3684e-01,  2.3120e-01,  5.2501e-01,\n",
      "         -5.1126e-02,  5.9431e-01, -2.5384e-01, -7.7006e-01, -9.0819e-01,\n",
      "          2.7635e-01, -3.0473e-01,  3.8711e-01,  7.8874e-01,  4.9177e-01,\n",
      "         -5.2304e-01,  5.8760e-01, -3.3973e-02, -6.9016e-01, -2.8483e-01,\n",
      "          4.8629e-01, -2.2030e-01, -4.1237e-01, -4.5023e-01,  7.1678e-01,\n",
      "         -2.8244e-01, -4.0377e-01, -4.2092e-01,  1.3112e-01, -1.7659e-02,\n",
      "         -4.7691e-01, -1.2792e+00,  7.0127e-02,  3.8000e-01,  3.6915e-01,\n",
      "          1.2045e+00,  8.1241e-02,  9.2300e-01,  1.3052e+00,  4.3584e-01,\n",
      "          2.2100e-01, -6.3524e-01, -7.5569e-01,  1.6211e-01,  3.1006e-01,\n",
      "         -5.4832e-01, -1.8871e-01, -4.1985e-01, -1.2600e-01, -2.3548e-01,\n",
      "         -3.5966e-01,  2.0529e-01,  1.1415e+00, -6.4137e-02,  9.7742e-02,\n",
      "         -7.8313e-01,  1.2928e-01, -4.7616e-03, -2.0667e-01,  7.6611e-01,\n",
      "         -7.0678e-01,  1.0317e+00,  6.0694e-03, -6.9554e-02, -4.7691e-01,\n",
      "          2.1342e-01, -8.1681e-01, -5.8532e-01,  2.3548e-01, -4.5534e-01,\n",
      "         -2.8320e-01,  2.3745e-02,  4.6163e-01, -1.0927e+00, -1.7002e+00,\n",
      "         -4.9084e-01,  1.5908e+00, -3.8954e-01, -1.5498e-01, -3.7795e-01,\n",
      "         -8.9226e-01,  8.4854e-02,  1.4967e-01, -2.3056e-01, -1.5898e-01,\n",
      "          1.5419e+00, -5.8530e-01,  6.1530e-01,  1.2822e+00,  6.7981e-01,\n",
      "         -3.8131e-01, -1.0654e+00,  6.6610e-01,  3.7984e-01, -8.1171e-02,\n",
      "         -4.0199e-01,  7.0593e-01, -4.2155e-01,  5.0527e-01,  3.9910e-01,\n",
      "          9.4304e-01,  7.9875e-01, -4.9037e-01, -9.5008e-01,  1.3495e-01,\n",
      "          1.7472e+00, -1.8713e-01, -2.9402e-01, -9.0907e-01,  1.2111e+00,\n",
      "          7.5041e-01,  9.5119e-02, -4.7905e-01,  5.9657e-02,  7.9059e-01,\n",
      "         -4.9105e-01, -8.2801e-01,  8.7420e-01,  9.3800e-01, -1.3977e-01,\n",
      "          5.3503e-01, -6.0993e-01,  1.2047e+00,  1.6724e-01,  1.3112e-01,\n",
      "          9.7551e-01, -7.3512e-01, -2.8745e-01, -2.2597e-01,  8.9845e-01,\n",
      "          2.4397e-01, -7.9873e-01,  6.2480e-01, -1.0967e+00, -9.9533e-01,\n",
      "         -3.6630e-01, -3.7455e-01,  9.4510e-01,  5.1869e-02, -1.9921e-02,\n",
      "         -6.2831e-01,  1.2678e+00,  1.4126e-01,  6.9623e-01, -4.4399e-01,\n",
      "         -9.2854e-01,  6.8433e-01,  9.3510e-01,  3.9186e-01, -7.7793e-01,\n",
      "          2.0377e-01,  1.8911e-01, -2.2742e-01, -2.5078e-01,  1.6882e-01,\n",
      "          6.9083e-01,  1.3505e-01,  3.1103e-01,  3.5420e-01, -7.3029e-01,\n",
      "          2.2683e-01,  2.0530e-01,  1.3288e-01,  3.0376e-01,  7.9016e-01,\n",
      "          1.8447e-01,  1.6436e-01,  7.0717e-01, -3.4959e-01, -9.3584e-01,\n",
      "          1.0837e+00,  7.3779e-01,  6.8133e-01,  8.6738e-01, -5.6696e-02,\n",
      "         -7.1048e-01, -8.4888e-01,  6.6503e-01,  8.1891e-01, -1.3830e+00,\n",
      "          6.8590e-01,  6.0768e-02,  7.8954e-01, -3.4041e-01, -2.1482e-01,\n",
      "         -3.7726e-01,  8.2266e-01, -1.3854e-01,  7.4984e-02,  8.5615e-03,\n",
      "         -3.4737e-01,  7.0525e-01, -4.1626e-02, -5.7488e-01,  1.0227e+00,\n",
      "         -1.4202e-01,  1.0776e-01, -2.6090e-03,  2.8479e-01,  3.6485e-01,\n",
      "          3.6910e-01, -1.1276e+00, -1.6168e-01,  6.3965e-03,  4.0335e-01,\n",
      "          3.7204e-01,  1.2370e+00,  5.4349e-01, -3.4269e-01,  8.7805e-01,\n",
      "         -1.2278e+00,  4.6898e-01, -1.7323e-01,  3.5354e-01,  8.5924e-01,\n",
      "         -7.8309e-01,  5.3819e-01,  8.8846e-01, -2.0094e-01, -9.9580e-01,\n",
      "          1.2556e-02, -3.6356e-01,  3.9787e-01, -7.2562e-01,  1.1320e+00,\n",
      "          1.1330e+00, -6.6112e-01, -7.8190e-01, -6.6905e-01,  3.6249e-02,\n",
      "          6.1955e-01, -9.0874e-02, -5.4330e-01,  2.0537e-01,  5.7668e-01,\n",
      "          1.3271e-01,  3.6872e-01, -1.3930e+00,  7.9022e-01, -6.1578e-01,\n",
      "         -3.0738e-01, -1.0625e-01, -7.1967e-02, -1.1669e+00, -1.8847e-01,\n",
      "         -5.7039e-01, -8.6491e-01,  2.9728e-01, -3.8045e-01,  1.5214e-01,\n",
      "         -3.4240e-01,  7.5644e-01, -2.7786e-01, -6.8418e-01,  4.7640e-01,\n",
      "         -2.1934e-02,  1.1352e+00,  1.1164e-01, -7.8807e-01,  1.3562e+00,\n",
      "          1.2750e-01, -1.1316e-01,  2.8891e-01, -4.4112e-01,  7.0714e-01,\n",
      "          2.3375e-01, -9.5553e-02,  1.6206e-01, -7.1263e-01,  3.1207e-01,\n",
      "         -1.6375e-01,  4.0865e-01,  7.6190e-01, -4.7270e-01,  2.7710e-01,\n",
      "          1.6578e+00, -8.7343e-02,  7.3154e-01, -9.0788e-01,  6.0927e-01,\n",
      "          2.7012e-01,  4.1590e-01, -2.9822e-01, -6.0368e-01,  8.8754e-01,\n",
      "         -8.7283e-01,  8.7391e-02,  1.4334e-01, -3.9612e-02, -2.0732e-01,\n",
      "         -5.3376e-01,  5.8845e-02, -4.9208e-01, -4.3782e-01, -3.8129e-01,\n",
      "          8.5416e-01,  3.3482e-01,  2.6813e-01, -5.5158e-01, -5.6504e-01,\n",
      "         -7.8798e-01, -2.7198e-01, -7.0521e-01,  1.0565e-01,  9.0544e-02,\n",
      "         -3.8920e-01, -1.7949e+00, -2.5044e-01, -1.1305e-01, -1.6536e+00,\n",
      "         -1.1596e+00,  5.3248e-01, -1.5503e+00,  4.7391e-01, -5.5946e-01,\n",
      "          4.8453e-01,  8.4567e-01,  6.6780e-01, -2.3529e-01,  6.1779e-02,\n",
      "          4.5135e-01, -6.4162e-02,  1.1625e-01,  1.8785e-01,  5.8788e-01,\n",
      "         -2.9859e-01, -2.6422e-01,  2.0927e-01, -1.5477e-01,  6.5452e-01,\n",
      "         -4.5142e-01,  1.2593e+00,  9.1867e-01,  7.5782e-01, -1.0238e+00,\n",
      "         -9.2970e-01, -7.5799e-02, -9.2130e-01,  1.0769e+00,  9.9830e-01,\n",
      "         -1.3444e+00, -1.3526e+00, -4.5777e-01,  1.7309e-01, -5.5609e-01,\n",
      "         -4.1302e-02, -6.2264e-01,  4.7874e-01, -4.4910e-01,  7.1212e-01,\n",
      "         -1.0285e+00, -5.7487e-01, -8.6619e-01, -3.6513e-01,  8.6421e-01,\n",
      "         -4.1502e-01,  1.6691e-01,  3.6624e-02, -7.1616e-01, -9.0512e-01,\n",
      "          7.9978e-01,  8.2313e-01,  1.5360e+00,  2.6914e-01,  7.2024e-01,\n",
      "         -2.0419e-01,  3.3087e-01, -7.2685e-01, -1.7813e-01, -3.8220e-01,\n",
      "          9.1484e-01,  7.1743e-01, -3.7667e-01, -2.9859e-01, -1.9247e-02,\n",
      "         -1.4195e-01,  5.7642e-01, -7.9673e-02,  9.9201e-01, -3.9519e-01,\n",
      "         -2.9830e-01, -6.4039e-01, -8.5935e-02, -9.9589e-01,  1.2763e-02,\n",
      "         -4.3865e-01, -4.4684e-01,  5.8352e-01,  3.1511e-01,  2.0011e-01,\n",
      "         -2.4527e-01, -1.2996e-01,  3.9788e-01, -5.5953e-01, -1.9935e-02,\n",
      "         -3.0291e-01, -7.1258e-01,  5.9339e-01,  5.4740e-01,  5.2652e-01,\n",
      "         -5.9219e-01, -2.6543e-01, -3.9395e-01, -2.6765e-01, -2.5368e-02,\n",
      "          4.7815e-01, -1.4641e+00,  1.3612e-01, -5.0214e-01, -2.5279e-01,\n",
      "          2.2745e-01, -1.3207e+00,  4.7487e-01, -4.3296e-01, -1.3101e-01,\n",
      "          3.3409e-01, -6.4021e-01, -4.6703e-01,  6.6784e-01, -2.5042e-01,\n",
      "          1.4155e-01,  3.5419e-02,  1.2342e+00, -1.3808e+00, -3.2155e-01,\n",
      "         -9.8017e-01, -2.4586e-01,  3.6702e-01, -3.7246e-01, -2.8684e-01,\n",
      "          5.6796e-01, -6.3793e-01, -8.7740e-02, -1.8441e-01,  7.0339e-01,\n",
      "          8.1589e-01, -1.3142e-01,  7.2028e-02,  4.6002e-01, -3.2760e-01,\n",
      "          4.2680e-01, -7.7848e-01,  8.2253e-01,  6.2701e-01, -1.3799e+00,\n",
      "         -3.7836e-01, -3.4407e-01,  5.8330e-01,  3.0518e-02, -1.0437e-01,\n",
      "         -1.6644e+00,  8.9975e-01,  2.8174e+00, -1.4790e-01,  5.9035e-01,\n",
      "          1.0415e-01,  1.4836e-01, -3.4857e-01, -2.8610e-01,  6.6696e-01,\n",
      "         -2.4394e-02, -9.5695e-01,  4.6371e-01, -7.7896e-01,  1.1154e+00,\n",
      "          3.3632e-01, -5.8035e-01,  1.0633e-01, -9.9017e-01,  1.6478e+00,\n",
      "          1.3281e+00, -3.4875e-01, -5.5119e-01, -3.3567e-01, -5.1112e-01,\n",
      "          7.8833e-01,  2.6175e-01, -4.8005e-01, -4.7043e-01,  4.8882e-01,\n",
      "          1.2883e+00,  8.2984e-01,  2.6116e-01, -5.7774e-01, -1.2482e+00,\n",
      "          8.1843e-03,  7.7395e-01,  5.1520e-01, -9.1438e-01,  6.7966e-01,\n",
      "          3.6416e-01,  5.1137e-01,  3.2991e-01,  4.8078e-01,  7.4820e-01,\n",
      "          2.2629e-02, -4.6850e-01,  2.3317e-01, -3.9121e-01,  2.7560e-01,\n",
      "         -5.6105e-01,  4.7713e-01, -2.4296e-01,  1.4676e+00,  9.1760e-02,\n",
      "          2.7710e-02, -3.6968e-01, -5.4117e-01, -2.7068e-01,  1.4119e+00,\n",
      "         -1.5873e+00, -4.7812e-01, -2.5659e-01, -3.9108e-01,  1.0213e+00,\n",
      "         -7.8509e-01, -7.8647e-01, -3.6625e-01,  3.6134e-01, -2.9059e-01,\n",
      "          2.2935e-01,  3.8257e-01, -2.8128e-01, -4.3816e-02, -8.6110e-01,\n",
      "         -2.5257e-01, -8.1779e-04, -7.5022e-01,  6.8236e-01,  5.3077e-01,\n",
      "          1.4469e+00, -2.7406e-01, -6.7573e-01,  4.3807e-01, -3.0593e-01,\n",
      "          3.5377e-01, -3.0970e-01, -1.0743e-01,  1.0839e+00, -3.2108e-01,\n",
      "         -1.3142e-01,  3.1948e-01,  1.7865e-01, -4.2577e-01,  3.4334e-01,\n",
      "          1.3305e-01, -1.8058e+00,  3.0836e-01, -8.1700e-02,  2.4322e-01,\n",
      "         -2.0381e-01, -8.0686e-01, -1.5947e+00, -1.2206e+00, -1.0841e+00,\n",
      "         -3.3522e-01, -2.7640e-01,  3.4030e-01,  4.9703e-01,  8.7475e-01,\n",
      "         -1.0912e+00, -6.9060e-01,  6.2873e-01, -2.4408e-01, -1.9926e-01,\n",
      "          7.8902e-02,  2.1130e-01, -6.8248e-01,  1.2950e+00, -5.2754e-01,\n",
      "         -4.5791e-01, -4.4563e-01, -5.3073e-01, -7.5511e-01,  8.2116e-01,\n",
      "         -6.7527e-01, -1.0495e+00,  7.5180e-02, -2.7439e-01, -5.6335e-01,\n",
      "          1.1550e-02,  5.0205e-01,  1.4317e-02,  1.6994e+00,  7.8682e-01,\n",
      "          3.7582e-01, -8.0774e-02, -6.0161e-01, -9.7074e-01, -3.8103e-01,\n",
      "          1.1774e+00, -9.1388e-01, -3.7785e-01,  2.0796e-01,  7.9943e-02,\n",
      "          1.4423e+00,  1.4283e-01, -3.4703e-01,  6.1253e-01, -1.7937e-01,\n",
      "         -6.7073e-01, -9.2170e-01,  6.3867e-01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = [\"量子コンピュータを使うデータサイエンティスト\"]\n",
    "sentence_embeddings = model.encode(sentences, batch_size=8)\n",
    "\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
