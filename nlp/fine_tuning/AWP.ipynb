{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWP(Adversarial Weight Perturbation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfRNTaLauu5T"
      },
      "source": [
        "* [AWPの実装](https://speakerdeck.com/masakiaota/kaggledeshi-yong-sarerudi-dui-xue-xi-fang-fa-awpnolun-wen-jie-shuo-toshi-zhuang-jie-shuo-adversarial-weight-perturbation-helps-robust-generalization?slide=30)\n",
        "* [amp実装の方の参考リンク](https://www.kaggle.com/code/wht1996/feedback-nn-train/notebook)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuyu-quant/Data_Science/blob/main/Natural_Language_processing/AWP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0EKjECExTse"
      },
      "source": [
        "## AWPの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W4YXmuuwJul"
      },
      "outputs": [],
      "source": [
        "# 修正する必要あり\n",
        "awp = AWP(model,\n",
        "          optimizer,\n",
        "          adv_lr=args.adv_lr,\n",
        "          adv_eps=args.adv_eps,\n",
        "          start_epoch=args.num_train_steps/args.epochs,\n",
        "          scaler=scaler\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwhIh29NwFFd"
      },
      "outputs": [],
      "source": [
        "# 修正する必要あり\n",
        "if f1_score > 0.64:\n",
        "    awp.attack_backward(input_ids,labels,attention_mask,step) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivj9APnPvrpr"
      },
      "outputs": [],
      "source": [
        "class AWP:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Module,\n",
        "        criterion: _Loss,\n",
        "        optimizer: Optimizer,\n",
        "        apex: bool,\n",
        "        adv_param: str=\"weight\",\n",
        "        adv_lr: float=1.0,\n",
        "        adv_eps: float=0.01\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.adv_param = adv_param\n",
        "        self.adv_lr = adv_lr\n",
        "        self.adv_eps = adv_eps\n",
        "        self.apex = apex\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}\n",
        "\n",
        "    def attack_backward(self, inputs: dict) -> Tensor:\n",
        "        with torch.cuda.amp.autocast(enabled=self.apex):\n",
        "            self._save()\n",
        "            self._attack_step() # モデルを近傍の悪い方へ改変\n",
        "            #y_preds = self.model(inputs)\n",
        "            _, _, adv_loss = self.model.training_step(inputs)\n",
        "            self.optimizer.zero_grad()\n",
        "        return adv_loss\n",
        "\n",
        "    def _attack_step(self) -> None:\n",
        "        e = 1e-6\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                norm1 = torch.norm(param.grad)\n",
        "                norm2 = torch.norm(param.data.detach())\n",
        "                if norm1 != 0 and not torch.isnan(norm1):\n",
        "                    # 直前に損失関数に通してパラメータの勾配を取得できるようにしておく必要あり\n",
        "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = torch.min(\n",
        "                        torch.max(\n",
        "                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
        "                    )\n",
        "\n",
        "    def _save(self) -> None:\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                if name not in self.backup:\n",
        "                    self.backup[name] = param.data.clone()\n",
        "                    grad_eps = self.adv_eps * param.abs().detach()\n",
        "                    self.backup_eps[name] = (\n",
        "                        self.backup[name] - grad_eps,\n",
        "                        self.backup[name] + grad_eps,\n",
        "                    )\n",
        "\n",
        "    def _restore(self) -> None:\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in self.backup:\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsnwqhiu4jS"
      },
      "source": [
        "## amp使用時のAWPの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UffrNOWyu5KI"
      },
      "outputs": [],
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "awp = AWP(model,\n",
        "          optimizer,\n",
        "          adv_lr=args.adv_lr,\n",
        "          adv_eps=args.adv_eps,\n",
        "          start_epoch=args.num_train_steps/args.epochs,\n",
        "          scaler=scaler\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtz9vpZruzoD"
      },
      "outputs": [],
      "source": [
        "# trainのループの中に書く\n",
        "# 単純計算で二倍の学習量がかかるため条件をつけてAWPを始めるのを遅らせる\n",
        "with torch.cuda.amp.autocast():\n",
        "    loss, tr_logits = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = loss.mean()\n",
        "\n",
        "\n",
        "optimizer.zero_grad()\n",
        "scaler.scale(loss).backward()\n",
        "\n",
        "if f1_score > 0.64:\n",
        "    awp.attack_backward(input_ids,labels,attention_mask,step) \n",
        "\n",
        "\n",
        "scaler.step(optimizer)\n",
        "scaler.update()\n",
        "scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgCshHUfui-X"
      },
      "outputs": [],
      "source": [
        "class AWP:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        optimizer,\n",
        "        adv_param=\"weight\",\n",
        "        adv_lr=1,\n",
        "        adv_eps=0.2,\n",
        "        start_epoch=0,\n",
        "        adv_step=1,\n",
        "        scaler=None\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.adv_param = adv_param\n",
        "        self.adv_lr = adv_lr\n",
        "        self.adv_eps = adv_eps\n",
        "        self.start_epoch = start_epoch\n",
        "        self.adv_step = adv_step\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def attack_backward(self, x, y, attention_mask,epoch):\n",
        "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
        "            return None\n",
        "\n",
        "        self._save() \n",
        "        for i in range(self.adv_step):\n",
        "            self._attack_step() \n",
        "            with torch.cuda.amp.autocast():\n",
        "                adv_loss, tr_logits = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n",
        "                adv_loss = adv_loss.mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            self.scaler.scale(adv_loss).backward()\n",
        "            \n",
        "        self._restore()\n",
        "    def _attack_step(self):\n",
        "        e = 1e-6\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                norm1 = torch.norm(param.grad)\n",
        "                norm2 = torch.norm(param.data.detach())\n",
        "                if norm1 != 0 and not torch.isnan(norm1):\n",
        "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = torch.min(\n",
        "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
        "                    )\n",
        "                # param.data.clamp_(*self.backup_eps[name])\n",
        "\n",
        "    def _save(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                if name not in self.backup:\n",
        "                    self.backup[name] = param.data.clone()\n",
        "                    grad_eps = self.adv_eps * param.abs().detach()\n",
        "                    self.backup_eps[name] = (\n",
        "                        self.backup[name] - grad_eps,\n",
        "                        self.backup[name] + grad_eps,\n",
        "                    )\n",
        "\n",
        "    def _restore(self,):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in self.backup:\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO9FNagznY3U+cFdtNovvMj",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
