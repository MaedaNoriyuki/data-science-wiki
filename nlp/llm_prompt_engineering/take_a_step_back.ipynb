{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a Step Back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMに推論するための概念や原理を与えることで正しく推論するための精度を向上させる手法．\n",
    "- 論文:https://arxiv.org/abs/2310.06117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fuyu-quant/data-science-wiki/blob/develop/nlp/llm_prompt_engineering/take_a_step_back.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanakatouma/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/langchain/llms/openai.py:202: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/tanakatouma/vscode/data-science-wiki/.venv/lib/python3.9/site-packages/langchain/llms/openai.py:790: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "llm_model = OpenAI(model_name= 'gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通常のプロンプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "温度が2倍、体積が8倍になると、理想気体の圧力Pはどうなりますか？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "理想気体の状態方程式はPV=nRTです。ここで、Pは圧力、Vは体積、nは物質量、Rは気体定数、Tは絶対温度です。\n",
      "\n",
      "この問題では、nとRは一定と考えられます。したがって、この方程式はP1V1/T1 = P2V2/T2と書くことができます。ここで、P1、V1、T1は初期の圧力、体積、温度で、P2、V2、T2は変化後の圧力、体積、温度です。\n",
      "\n",
      "問題から、T2 = 2T1、V2 = 8V1であることがわかります。これを上記の方程式に代入して、P2を求めると、\n",
      "\n",
      "P2 = P1 * (V2/V1) * (T1/T2) = P1 * (8V1/V1) * (T1/2T1) = 4P1\n",
      "\n",
      "したがって、温度が2倍、体積が8倍になると、理想気体の圧力は4倍になります。\n"
     ]
    }
   ],
   "source": [
    "output = llm_model(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"\"\"\n",
    "Q:温度が2倍、体積が8倍になると、理想気体の圧力Pはどうなりますか？\n",
    "---------\n",
    "上記の質問の背景には何がありますか？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この質問は理想気体の法則、特にゲイ＝ルサックの法則とボイル＝シャルルの法則に基づいています。理想気体の法則は、気体の圧力、体積、温度、および気体の量の間の関係を記述します。ゲイ＝ルサックの法則は、一定の体積での気体の圧力は絶対温度に比例すると述べています。一方、ボイル＝シャルルの法則は、一定の温度での気体の圧力は体積に反比例すると述べています。したがって、この質問はこれらの法則を適用して、温度と体積が変化したときの気体の圧力の変化を求めています。\n"
     ]
    }
   ],
   "source": [
    "output1 = llm_model(prompt1)\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"\"\"\n",
    "背景:{output1_}\n",
    "Q:温度が2倍、体積が8倍になると、理想気体の圧力Pはどうなりますか？\n",
    "A:\n",
    "\"\"\".format(output1_ = output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ゲイ＝ルサックの法則によれば、温度が2倍になると、圧力も2倍になります。一方、ボイル＝シャルルの法則によれば、体積が8倍になると、圧力は1/8になります。したがって、これらの変化を組み合わせると、圧力は2倍と1/8の積、つまり1/4になります。\n"
     ]
    }
   ],
   "source": [
    "output2 = llm_model(prompt2)\n",
    "print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17a011378fed683b21aba93e5dd7c0cb7beefc09c5af72c6425b40c713e260dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
