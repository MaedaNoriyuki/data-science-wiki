{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242ce3ba-7a96-46ce-b599-f30c2031e101",
   "metadata": {},
   "source": [
    "# MMBT(MultiModal BiTransformers)\n",
    "* [Github](https://github.com/facebookresearch/mmbt)\n",
    "* 事前学習済みのBERTとResNet152を使っていて，それらの出力をさらにBERTに入力する\n",
    "* BERT単体，ResNet単体のモデルや両方のモデルを単純に結合した場合より高い精度が出る\n",
    "* 参考資料  \n",
    "https://qiita.com/toshiyuki_tsutsui/items/a01e2a3ffae035ef644c  \n",
    "https://qiita.com/toshiyuki_tsutsui/items/68b77e62b06af08b7399  \n",
    "https://wwacky.hateblo.jp/entry/2020/04/05/230134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e84b276-c1e9-4465-81d8-1510ed3e37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841491a7-b92a-4af3-a627-1de19f36dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "#from mmbt.models.image import ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c79e3-fd7a-47a4-bba6-7f9e6067f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        model = torchvision.models.resnet152(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "        pool_func = (\n",
    "            nn.AdaptiveAvgPool2d\n",
    "            if args.img_embed_pool_type == \"avg\"\n",
    "            else nn.AdaptiveMaxPool2d\n",
    "        )\n",
    "\n",
    "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
    "            self.pool = pool_func((args.num_image_embeds, 1))\n",
    "        elif args.num_image_embeds == 4:\n",
    "            self.pool = pool_func((2, 2))\n",
    "        elif args.num_image_embeds == 6:\n",
    "            self.pool = pool_func((3, 2))\n",
    "        elif args.num_image_embeds == 8:\n",
    "            self.pool = pool_func((4, 2))\n",
    "        elif args.num_image_embeds == 9:\n",
    "            self.pool = pool_func((3, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
    "        out = self.pool(self.model(x))\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out  # BxNx2048\n",
    "\n",
    "\n",
    "class ImageClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.img_encoder = ImageEncoder(args)\n",
    "        self.clf = nn.Linear(args.img_hidden_sz * args.num_image_embeds, args.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.img_encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        out = self.clf(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810a97a-ca33-4b3e-8b66-283a31759a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "from mmbt.models.image import ImageEncoder\n",
    "\n",
    "\n",
    "class ImageBertEmbeddings(nn.Module):\n",
    "    def __init__(self, args, embeddings):\n",
    "        super(ImageBertEmbeddings, self).__init__()\n",
    "        self.args = args\n",
    "        self.img_embeddings = nn.Linear(args.img_hidden_sz, args.hidden_sz)\n",
    "        self.position_embeddings = embeddings.position_embeddings\n",
    "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
    "        self.word_embeddings = embeddings.word_embeddings\n",
    "        self.LayerNorm = embeddings.LayerNorm\n",
    "        self.dropout = nn.Dropout(p=args.dropout)\n",
    "\n",
    "    def forward(self, input_imgs, token_type_ids):\n",
    "        bsz = input_imgs.size(0)\n",
    "        seq_length = self.args.num_image_embeds + 2  # +2 for CLS and SEP Token\n",
    "\n",
    "        cls_id = torch.LongTensor([self.args.vocab.stoi[\"[CLS]\"]]).cuda()\n",
    "        cls_id = cls_id.unsqueeze(0).expand(bsz, 1)\n",
    "        cls_token_embeds = self.word_embeddings(cls_id)\n",
    "\n",
    "        sep_id = torch.LongTensor([self.args.vocab.stoi[\"[SEP]\"]]).cuda()\n",
    "        sep_id = sep_id.unsqueeze(0).expand(bsz, 1)\n",
    "        sep_token_embeds = self.word_embeddings(sep_id)\n",
    "\n",
    "        imgs_embeddings = self.img_embeddings(input_imgs)\n",
    "        token_embeddings = torch.cat(\n",
    "            [cls_token_embeds, imgs_embeddings, sep_token_embeds], dim=1\n",
    "        )\n",
    "\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).cuda()\n",
    "        position_ids = position_ids.unsqueeze(0).expand(bsz, seq_length)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class MultimodalBertEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalBertEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        bert = BertModel.from_pretrained(args.bert_model)\n",
    "        self.txt_embeddings = bert.embeddings\n",
    "\n",
    "        if args.task == \"vsnli\":\n",
    "            ternary_embeds = nn.Embedding(3, args.hidden_sz)\n",
    "            ternary_embeds.weight.data[:2].copy_(\n",
    "                bert.embeddings.token_type_embeddings.weight\n",
    "            )\n",
    "            ternary_embeds.weight.data[2].copy_(\n",
    "                bert.embeddings.token_type_embeddings.weight.data.mean(dim=0)\n",
    "            )\n",
    "            self.txt_embeddings.token_type_embeddings = ternary_embeds\n",
    "\n",
    "        self.img_embeddings = ImageBertEmbeddings(args, self.txt_embeddings)\n",
    "        self.img_encoder = ImageEncoder(args)\n",
    "        self.encoder = bert.encoder\n",
    "        self.pooler = bert.pooler\n",
    "        self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n",
    "\n",
    "    def forward(self, input_txt, attention_mask, segment, input_img):\n",
    "        bsz = input_txt.size(0)\n",
    "        attention_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones(bsz, self.args.num_image_embeds + 2).long().cuda(),\n",
    "                attention_mask,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=next(self.parameters()).dtype\n",
    "        )\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        img_tok = (\n",
    "            torch.LongTensor(input_txt.size(0), self.args.num_image_embeds + 2)\n",
    "            .fill_(0)\n",
    "            .cuda()\n",
    "        )\n",
    "        img = self.img_encoder(input_img)  # BxNx3x224x224 -> BxNx2048\n",
    "        img_embed_out = self.img_embeddings(img, img_tok)\n",
    "        txt_embed_out = self.txt_embeddings(input_txt, segment)\n",
    "        encoder_input = torch.cat([img_embed_out, txt_embed_out], 1)  # Bx(TEXT+IMG)xHID\n",
    "\n",
    "        encoded_layers = self.encoder(\n",
    "            encoder_input, extended_attention_mask, output_all_encoded_layers=False\n",
    "        )\n",
    "\n",
    "        return self.pooler(encoded_layers[-1])\n",
    "\n",
    "\n",
    "class MultimodalBertClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalBertClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.enc = MultimodalBertEncoder(args)\n",
    "        self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n",
    "\n",
    "    def forward(self, txt, mask, segment, img):\n",
    "        x = self.enc(txt, mask, segment, img)\n",
    "        return self.clf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9fef1-1bf2-4e34-897d-89a4ed9c81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalConcatBertClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalConcatBertClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.txtenc = BertEncoder(args)\n",
    "        self.imgenc = ImageEncoder(args)\n",
    "\n",
    "        last_size = args.hidden_sz + (args.img_hidden_sz * args.num_image_embeds)\n",
    "        self.clf = nn.ModuleList()\n",
    "        for hidden in args.hidden:\n",
    "            self.clf.append(nn.Linear(last_size, hidden))\n",
    "            if args.include_bn:\n",
    "                self.clf.append(nn.BatchNorm1d(hidden))\n",
    "            self.clf.append(nn.ReLU())\n",
    "            self.clf.append(nn.Dropout(args.dropout))\n",
    "            last_size = hidden\n",
    "\n",
    "        self.clf.append(nn.Linear(last_size, args.n_classes))\n",
    "\n",
    "    def forward(self, txt, mask, segment, img):\n",
    "        txt = self.txtenc(txt, mask, segment)\n",
    "        img = self.imgenc(img)\n",
    "        img = torch.flatten(img, start_dim=1)\n",
    "        out = torch.cat([txt, img], -1)\n",
    "        for layer in self.clf:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77fc3b-9c56-4348-8c6b-3c8a2de3e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.bert = BertModel.from_pretrained(args.bert_model)\n",
    "\n",
    "    def forward(self, txt, mask, segment):\n",
    "        _, out = self.bert(\n",
    "            txt,\n",
    "            token_type_ids=segment,\n",
    "            attention_mask=mask,\n",
    "            output_all_encoded_layers=False,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(BertClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.enc = BertEncoder(args)\n",
    "        self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n",
    "        self.clf.apply(self.enc.bert.init_bert_weights)\n",
    "\n",
    "    def forward(self, txt, mask, segment):\n",
    "        x = self.enc(txt, mask, segment)\n",
    "        return self.clf(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
