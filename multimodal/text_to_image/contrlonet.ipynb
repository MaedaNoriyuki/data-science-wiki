{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qq opencv-contrib-python diffusers transformers git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image(\"https://cdn-ak.f.st-hatena.com/images/fotolife/a/aa_debdeb/20160714/20160714112033.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "np_image = np.array(image)\n",
    "\n",
    "canny_image = cv2.Canny(np_image, low_threshold, high_threshold)\n",
    "\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "import torch\n",
    "\n",
    "# Canny Edge用のControlNetを指定　（このモデルは固定）\n",
    "controlnet_model = \"lllyasviel/sd-controlnet-canny\"\n",
    "\n",
    "# 好きなStable Diffusionを使用 (e.g. huggingface.coとかcivitai.com)\n",
    "sd_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# ControlNetモデルのダウンロード\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_model,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Stable Diffusion & ControlNet パイプラインの構築\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    sd_model,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 多く試してはいないが、スケジューラーによって結果が変わった\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"wonder woman in DC comics\"\n",
    "num_steps = 20\n",
    "seed = 0\n",
    "\n",
    "out_image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=torch.manual_seed(seed),\n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"prompt: {prompt}\")\n",
    "\n",
    "Image.fromarray(np.concatenate([image, canny_image, out_image], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Depth-to-Imageではdepth-estimationというモデルを追加でロード\n",
    "depth_estimator = pipeline('depth-estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://i.imgur.com/2V9gTol.jpg\")\n",
    "# image = load_image(\"http://file.mk.co.kr/meet/neds/2021/06/image_readtop_2021_583221_16238253014683484.jpg\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_image = depth_estimator(image)['depth']\n",
    "depth_image = np.array(depth_image)\n",
    "depth_image = depth_image[:, :, None]\n",
    "depth_image = np.concatenate([depth_image, depth_image, depth_image], axis=2)\n",
    "depth_image = Image.fromarray(depth_image)\n",
    "\n",
    "depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth用のControlNetを指定　（このモデルは固定）\n",
    "controlnet_model = \"fusing/stable-diffusion-v1-5-controlnet-depth\"\n",
    "\n",
    "# アニメ風にするモデル (https://huggingface.co/andite/anything-v4.0)\n",
    "sd_model = \"andite/anything-v4.0\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_model,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    sd_model,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 他のスケジューラーも使ってみましょう\n",
    "# 作ったpipeに使用できるスケジューラーを表示\n",
    "pipe.scheduler.compatibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DPMSolverMultistepScheduler, HeunDiscreteScheduler, DDPMScheduler, EulerDiscreteScheduler, KDPM2DiscreteScheduler, EulerAncestralDiscreteScheduler, DEISMultistepScheduler, KDPM2AncestralDiscreteScheduler, UniPCMultistepScheduler, LMSDiscreteScheduler, PNDMScheduler, DPMSolverSinglestepScheduler\n",
    "\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"scenery, mountain, village, outdoors, sky, clouds\"\n",
    "\n",
    "num_steps = 50\n",
    "guidance_scale = 14\n",
    "seed = 1\n",
    "\n",
    "out_image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=torch.manual_seed(seed),\n",
    "    image=depth_image,\n",
    ").images[0]\n",
    "\n",
    "out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"prompt: {prompt}\")\n",
    "\n",
    "Image.fromarray(np.concatenate([image.resize(out_image.size), depth_image.resize(out_image.size), out_image], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq controlnet_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://i.imgur.com/uJpMDyu.jpg\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pointなし(non-ema)のStable Diffusionモデルをダウンロード\n",
    "!git clone -b non-ema --single-branch https://huggingface.co/runwayml/stable-diffusion-v1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# civitaiからweightだけをダウンロードしてStable Diffusionモデルの配下に保存\n",
    "!wget https://civitai.com/api/download/models/7974 -O stable-diffusion-v1-5/etherRealMixERM_etherrealmixERMV1.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "# Open-Pose用のControlNetを指定（このモデルは固定）\n",
    "controlnet_model = \"fusing/stable-diffusion-v1-5-controlnet-openpose\"\n",
    "\n",
    "# Stable Diffusionモデルになっているが、実際に使うのはダウンロードしたEther Real Mix - ERMモデル\n",
    "sd_model = \"/content/stable-diffusion-v1-5\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_model,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    sd_model,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler.compatibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DPMSolverMultistepScheduler, HeunDiscreteScheduler, DDPMScheduler, EulerDiscreteScheduler, KDPM2DiscreteScheduler, EulerAncestralDiscreteScheduler, DEISMultistepScheduler, KDPM2AncestralDiscreteScheduler, UniPCMultistepScheduler, LMSDiscreteScheduler, PNDMScheduler, DPMSolverSinglestepScheduler\n",
    "\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"RAW photo, a portrait photo of 50 y.o japanese man in clothes, night tokyo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"\n",
    "prompt = \"photo-realistic, may sometimes affect the quality if you give it a high weight, which makes the final picture looks like scattering glasses.omertosa,head \"\n",
    "negative_prompt = \"NSFW, (worst quality, low quality:1.3), watermark, signature\"\n",
    "num_steps = 28\n",
    "guidance_scale = 7\n",
    "seed = 51251555\n",
    "\n",
    "out_image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=torch.manual_seed(seed),\n",
    "    image=pose_image,\n",
    ").images[0]\n",
    "\n",
    "out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"prompt: {prompt}\")\n",
    "\n",
    "Image.fromarray(np.concatenate([image.resize(out_image.size), pose_image.resize(out_image.size), out_image], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import DDIMScheduler, DPMSolverMultistepScheduler, HeunDiscreteScheduler, DDPMScheduler, EulerDiscreteScheduler, KDPM2DiscreteScheduler, EulerAncestralDiscreteScheduler, DEISMultistepScheduler, KDPM2AncestralDiscreteScheduler, UniPCMultistepScheduler, LMSDiscreteScheduler, PNDMScheduler, DPMSolverSinglestepScheduler\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from diffusers import DDPMScheduler\n",
    "import torch\n",
    "import random, sys\n",
    "\n",
    "controlnet_model = \"lllyasviel/sd-controlnet-canny\"\n",
    "sd_model = \"Lykon/DreamShaper\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_model,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    sd_model,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "def img2img(img_path, prompt, negative_prompt, num_steps=20, guidance_scale=7, seed=0, low=100, high=200):\n",
    "    image = load_image(img_path)\n",
    "\n",
    "    np_image = np.array(image)\n",
    "\n",
    "    canny_image = cv2.Canny(np_image, low, high)\n",
    "\n",
    "    canny_image = canny_image[:, :, None]\n",
    "    canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "    canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "    out_image = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.manual_seed(seed),\n",
    "        image=canny_image\n",
    "    ).images[0]\n",
    "\n",
    "    return image, canny_image, out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"masterpiece, best quality, ultra-detailed, illustration, school uniform, scarf, gymnasium\"\n",
    "negative_prompt = \"lowres, ((bad anatomy)), ((bad hands)), text, missing finger, extra digits, fewer digits, blurry, ((mutated hands and fingers)), (poorly drawn face), ((mutation)), ((deformed face)), (ugly), ((bad proportions)), ((extra limbs)), extra face, (double head), (extra head), ((extra feet)), monster, logo, cropped, worst quality, low quality, normal quality, jpeg, humpbacked, long body, long neck, ((jpeg artifacts))\"\n",
    "num_steps = 20\n",
    "guidance_scale = 7\n",
    "seed = 3467120481370323442\n",
    "\n",
    "img_path = \"https://post-phinf.pstatic.net/MjAyMzAzMDlfMjcy/MDAxNjc4MjkxNjU5MzE4.2VqU5CdXbJlPO7YIFkya7qnKIEsngQ1yfehnuZb0y_Eg.ZCAqiwDzQfSQ8_5PTPklTl9DFCRC3eUua_v3HfrvsyUg.PNG/image_8486401811678287934915.png?type=w1200\"\n",
    "\n",
    "image, canny_image, out_image = img2img(img_path, prompt, negative_prompt, num_steps, guidance_scale, seed, low=50, high=100)\n",
    "\n",
    "out_image.save(\"02_result.png\")\n",
    "Image.fromarray(np.concatenate([image.resize(out_image.size), canny_image.resize(out_image.size), out_image], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"masterpiece, best quality, ultra-detailed, illustration, a man sitting, a lady sitting, tree, autumn, fallen leaves\"\n",
    "negative_prompt = \"NSFW, lowres, ((bad anatomy)), ((bad hands)), text, missing finger, extra digits, fewer digits, blurry, ((mutated hands and fingers)), (poorly drawn face), ((mutation)), ((deformed face)), (ugly), ((bad proportions)), ((extra limbs)), extra face, (double head), (extra head), ((extra feet)), monster, logo, cropped, worst quality, low quality, normal quality, jpeg, humpbacked, long body, long neck, ((jpeg artifacts))\"\n",
    "num_steps = 20\n",
    "guidance_scale = 7\n",
    "seed = 3311352378018201782\n",
    "\n",
    "img_path = \"https://post-phinf.pstatic.net/MjAyMzAzMDlfMTI2/MDAxNjc4MjkxNjY4MDY1.VTeW8x873IE03L2xJi5NDr1HGskSA4Y3Zws_aN68DEsg.tsx75T60hL6ejyzMnzo7kxfj1VK_f0VzFONY56T0VtMg.JPEG/image_8366549961678289357607.jpg?type=w1200\"\n",
    "\n",
    "image, canny_image, out_image = img2img(img_path, prompt, negative_prompt, num_steps, guidance_scale, seed, 50, 80)\n",
    "\n",
    "out_image.save(\"12_result.png\")\n",
    "Image.fromarray(np.concatenate([image.resize(out_image.size), canny_image.resize(out_image.size), out_image], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"masterpiece, best quality, ultra-detailed, illustration, a girl with backpack, a man down on his knee\"\n",
    "negative_prompt = \"NSFW, lowres, ((bad anatomy)), ((bad hands)), text, missing finger, extra digits, fewer digits, blurry, ((mutated hands and fingers)), (poorly drawn face), ((mutation)), ((deformed face)), (ugly), ((bad proportions)), ((extra limbs)), extra face, (double head), (extra head), ((extra feet)), monster, logo, cropped, worst quality, low quality, normal quality, jpeg, humpbacked, long body, long neck, ((jpeg artifacts))\"\n",
    "num_steps = 20\n",
    "guidance_scale = 7\n",
    "seed = 3465640518011203857\n",
    "\n",
    "img_path = \"https://post-phinf.pstatic.net/MjAyMzAzMDlfMjQg/MDAxNjc4Mjk0MjE1MTMy.IQ5DuY2RIoErY_o-wA73wdWyVgMM8VtbkWgDI5nyw_8g.9lJrV2mwz_yvPOFSKlb6Hg_TWYrpryTTtwxiTDx08uUg.JPEG/image_888375631678289902427.jpg?type=w1200\"\n",
    "\n",
    "image, canny_image, out_image = img2img(img_path, prompt, negative_prompt, num_steps, guidance_scale, seed, 50, 100)\n",
    "\n",
    "out_image.save(\"14_result.png\")\n",
    "Image.fromarray(np.concatenate([image.resize(out_image.size), canny_image.resize(out_image.size), out_image], axis=1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
