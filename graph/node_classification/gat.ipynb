{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmllc9VsuVL0"
   },
   "source": [
    "# Graph Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fuyu-quant/Data_Science/blob/main/Graph/node_classification/gat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SqHJ0riWuTjJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
    "!pip install torch-geometric\n",
    "!pip install scipy==1.8.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "omalo5IJuYc2"
   },
   "outputs": [],
   "source": [
    "# GCN\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "# データセット\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "# グラフ用DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "# グラフの可視化ツール\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe4l6ts4uxF2",
    "outputId": "e0b20409-d185-4373-bc25-dddbe5d1a6d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MUTAG(188)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TUDataset(root=\"/tmp/MUTAG\", name=\"MUTAG\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVu4KGgU1_UM",
    "outputId": "d226ee38-e560-48f6-b510-170420a6d17d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BBvT9Mfiuy7B"
   },
   "outputs": [],
   "source": [
    "def graph_info(data):\n",
    "    print(\"ノードの数:\", data.num_nodes)\n",
    "    print(\"エッジの数:\", data.num_edges)\n",
    "    print(\"特徴量の数:\", data.num_node_features)\n",
    "    print(\"無向グラフか？:\", data.is_undirected())\n",
    "    print(\"孤立したノードが有るか？:\", data.has_isolated_nodes())\n",
    "    print(\"自己ループがあるか？:\", data.has_self_loops())\n",
    "    print(\"キー: \", data.keys)\n",
    "    print(\"各ノードの特徴量\")\n",
    "    print(data[\"x\"])\n",
    "    print(\"各ノードのラベル\")\n",
    "    print(data[\"y\"])\n",
    "    print(\"各エッジ\")\n",
    "    print(data[\"edge_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyQA4DpDu1Nw",
    "outputId": "5da7455b-7e60-43d6-8da5-21479f8c7ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ノードの数: 17\n",
      "エッジの数: 38\n",
      "特徴量の数: 7\n",
      "無向グラフか？: True\n",
      "孤立したノードが有るか？: False\n",
      "自己ループがあるか？: False\n",
      "キー:  ['edge_index', 'edge_attr', 'x', 'y']\n",
      "各ノードの特徴量\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.]])\n",
      "各ノードのラベル\n",
      "tensor([1])\n",
      "各エッジ\n",
      "tensor([[ 0,  0,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  6,  6,  7,  7,\n",
      "          8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14,\n",
      "         15, 16],\n",
      "        [ 1,  5,  0,  2,  1,  3,  2,  4,  9,  3,  5,  6,  0,  4,  4,  7,  6,  8,\n",
      "          7,  9, 13,  3,  8, 10,  9, 11, 10, 12, 11, 13, 14,  8, 12, 12, 15, 16,\n",
      "         14, 14]])\n"
     ]
    }
   ],
   "source": [
    "graph_info(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5i4HoQwgu5M3",
    "outputId": "da00a6aa-200c-4212-9f08-688e844fd2c1"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4238\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4239\u001b[0;31m                 \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4240\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Test dimensionality to reject single floats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid RGBA argument: {orig_c!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;31m# Return a tuple to prevent the cached value from being modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: 1.0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5e9d954271f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m nx.draw(data_nx,\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnode_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         node_size=10)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"with_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"labels\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mdraw_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# default to spring layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mdraw_networkx_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnode_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0mdraw_networkx_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0medge_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx_nodes\u001b[0;34m(G, pos, nodelist, node_size, node_color, node_shape, alpha, cmap, vmin, vmax, ax, linewidths, edgecolors, label, margins)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     node_collection = ax.scatter(\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4400\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4401\u001b[0;31m             self._parse_scatter_color_args(\n\u001b[0m\u001b[1;32m   4402\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4240\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4241\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4242\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0minvalid_shape_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4243\u001b[0m                 \u001b[0;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m                 \u001b[0;31m# severe failure => one may appreciate a verbose feedback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'c' argument has 1 elements, which is inconsistent with 'x' and 'y' with size 17."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset[0].cpu()\n",
    "data_nx = to_networkx(data) # networkxのグラフに変換\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "nx.draw(data_nx,\n",
    "        node_color = data.y,\n",
    "        node_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqxvROAxvzD3",
    "outputId": "20a4397d-0b71-4462-fb2b-845f830efb7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Extracting /tmp/MUTAG/MUTAG/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root=\"/tmp/MUTAG\", name=\"MUTAG\")\n",
    "\n",
    "dataset = dataset.shuffle()  # データセットをシャッフル\n",
    "dataset_train = dataset[:140]  # 訓練用データセット\n",
    "dataset_test = dataset[140:]  # テスト用データセット\n",
    "\n",
    "batch_size = 64  # バッチサイズ\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ce9UcCT3v4E-"
   },
   "outputs": [],
   "source": [
    "n_h = 64  # 中間層における特徴量の数\n",
    "n_head = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS_28_Nlvknl",
    "outputId": "bd1004e7-058a-4694-f074-9e40010538e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (gat1): GATConv(7, 64, heads=32)\n",
       "  (gat2): GATConv(2048, 64, heads=32)\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(dataset.num_node_features,\n",
    "                            n_h,\n",
    "                            heads=n_head)\n",
    "        self.gat2 = GATConv(n_h*n_head,\n",
    "                            n_h,\n",
    "                            heads=n_head)\n",
    "        self.fc = nn.Linear(n_h*n_head, dataset.num_classes)  # 全結合層\n",
    "\n",
    "        self.relu = nn.ReLU()  # ReLU\n",
    "        self.dropout = nn.Dropout(p=0.5)  # ドロップアウト:(p=ドロップアウト率)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 全てのノードで各特徴量の平均をとる\n",
    "        x = global_mean_pool(x, batch)  # (バッチサイズ, 特徴量の数)に変換\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = GAT()\n",
    "net.cuda()  #GPU対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XXrSlvRWvm3t"
   },
   "outputs": [],
   "source": [
    "def eval(loader):\n",
    "    correct = 0  # 正解数\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.cuda()  # GPU対応\n",
    "        out = net(data)  \n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "\n",
    "    return correct/len(loader.dataset)  # 正解率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9VSk5C-vpDY",
    "outputId": "1ad27faf-e16e-4399-f169-207144188a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 1 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 2 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 3 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 4 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 5 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 6 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 7 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 8 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 9 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 10 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 11 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 12 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 13 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 14 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 15 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 16 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 17 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 18 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 19 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 20 acc_train: 87.85714285714286% acc_test: 81.25%\n",
      "Epoch: 21 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 22 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 23 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 24 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 25 acc_train: 86.42857142857143% acc_test: 72.91666666666666%\n",
      "Epoch: 26 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 27 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 28 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 29 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 30 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 31 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 32 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 33 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 34 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 35 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 36 acc_train: 90.0% acc_test: 79.16666666666666%\n",
      "Epoch: 37 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 38 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 39 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 40 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 41 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 42 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 43 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 44 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 45 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 46 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 47 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 48 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 49 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 50 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 51 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 52 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 53 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 54 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 55 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 56 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 57 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 58 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 59 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 60 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 61 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 62 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 63 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 64 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 65 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 66 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 67 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 68 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 69 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 70 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 71 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 72 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 73 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 74 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 75 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 76 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 77 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 78 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 79 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 80 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 81 acc_train: 86.42857142857143% acc_test: 79.16666666666666%\n",
      "Epoch: 82 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 83 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 84 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 85 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 86 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 87 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 88 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 89 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 90 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 91 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 92 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 93 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 94 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 95 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 96 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 97 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 98 acc_train: 86.42857142857143% acc_test: 75.0%\n",
      "Epoch: 99 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 100 acc_train: 90.0% acc_test: 79.16666666666666%\n",
      "Epoch: 101 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 102 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 103 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 104 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 105 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 106 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 107 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 108 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 109 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 110 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 111 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 112 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 113 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 114 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 115 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 116 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 117 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 118 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 119 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 120 acc_train: 86.42857142857143% acc_test: 79.16666666666666%\n",
      "Epoch: 121 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 122 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 123 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 124 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 125 acc_train: 86.42857142857143% acc_test: 79.16666666666666%\n",
      "Epoch: 126 acc_train: 86.42857142857143% acc_test: 77.08333333333334%\n",
      "Epoch: 127 acc_train: 86.42857142857143% acc_test: 75.0%\n",
      "Epoch: 128 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 129 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 130 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 131 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 132 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 133 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 134 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 135 acc_train: 90.0% acc_test: 79.16666666666666%\n",
      "Epoch: 136 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 137 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 138 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 139 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 140 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 141 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 142 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 143 acc_train: 90.0% acc_test: 79.16666666666666%\n",
      "Epoch: 144 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 145 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 146 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 147 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 148 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 149 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 150 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 151 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 152 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 153 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 154 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 155 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 156 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 157 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 158 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 159 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 160 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 161 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 162 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 163 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 164 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 165 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 166 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 167 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 168 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 169 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 170 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 171 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 172 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 173 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 174 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 175 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 176 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 177 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 178 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 179 acc_train: 86.42857142857143% acc_test: 77.08333333333334%\n",
      "Epoch: 180 acc_train: 85.71428571428571% acc_test: 77.08333333333334%\n",
      "Epoch: 181 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 182 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 183 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 184 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 185 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 186 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 187 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 188 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 189 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 190 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 191 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 192 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 193 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 194 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 195 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 196 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 197 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 198 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 199 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 200 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 201 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 202 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 203 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 204 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 205 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 206 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 207 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 208 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 209 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 210 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 211 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 212 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 213 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 214 acc_train: 90.0% acc_test: 77.08333333333334%\n",
      "Epoch: 215 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 216 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 217 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 218 acc_train: 87.14285714285714% acc_test: 72.91666666666666%\n",
      "Epoch: 219 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 220 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 221 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 222 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 223 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 224 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 225 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 226 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 227 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 228 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 229 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 230 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 231 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 232 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 233 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 234 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 235 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 236 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 237 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 238 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 239 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 240 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 241 acc_train: 87.14285714285714% acc_test: 75.0%\n",
      "Epoch: 242 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 243 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 244 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 245 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 246 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 247 acc_train: 87.14285714285714% acc_test: 79.16666666666666%\n",
      "Epoch: 248 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 249 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 250 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 251 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 252 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 253 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 254 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 255 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 256 acc_train: 90.0% acc_test: 79.16666666666666%\n",
      "Epoch: 257 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 258 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 259 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 260 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 261 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 262 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 263 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 264 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 265 acc_train: 87.14285714285714% acc_test: 72.91666666666666%\n",
      "Epoch: 266 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 267 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 268 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 269 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 270 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 271 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 272 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 273 acc_train: 87.14285714285714% acc_test: 72.91666666666666%\n",
      "Epoch: 274 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 275 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 276 acc_train: 87.14285714285714% acc_test: 77.08333333333334%\n",
      "Epoch: 277 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 278 acc_train: 87.85714285714286% acc_test: 75.0%\n",
      "Epoch: 279 acc_train: 87.85714285714286% acc_test: 72.91666666666666%\n",
      "Epoch: 280 acc_train: 88.57142857142857% acc_test: 72.91666666666666%\n",
      "Epoch: 281 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 282 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 283 acc_train: 87.85714285714286% acc_test: 77.08333333333334%\n",
      "Epoch: 284 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 285 acc_train: 87.85714285714286% acc_test: 79.16666666666666%\n",
      "Epoch: 286 acc_train: 89.28571428571429% acc_test: 77.08333333333334%\n",
      "Epoch: 287 acc_train: 89.28571428571429% acc_test: 72.91666666666666%\n",
      "Epoch: 288 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 289 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 290 acc_train: 90.0% acc_test: 72.91666666666666%\n",
      "Epoch: 291 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 292 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 293 acc_train: 88.57142857142857% acc_test: 79.16666666666666%\n",
      "Epoch: 294 acc_train: 89.28571428571429% acc_test: 79.16666666666666%\n",
      "Epoch: 295 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n",
      "Epoch: 296 acc_train: 88.57142857142857% acc_test: 75.0%\n",
      "Epoch: 297 acc_train: 90.0% acc_test: 75.0%\n",
      "Epoch: 298 acc_train: 89.28571428571429% acc_test: 75.0%\n",
      "Epoch: 299 acc_train: 88.57142857142857% acc_test: 77.08333333333334%\n"
     ]
    }
   ],
   "source": [
    "# 交差エントロピー誤差関数\n",
    "loss_fnc = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "for epoch in range(300):\n",
    "    # 訓練\n",
    "    net.train()  # 訓練モード\n",
    "    for data in loader_train: \n",
    "        data = data.cuda()  # GPU対応\n",
    "\n",
    "        optimizer.zero_grad()  # ①勾配の初期化\n",
    "        out = net(data)  # ②順伝播により予測値を得る\n",
    "        loss = loss_fnc(out, data.y)  # ③予測値と正解値から誤差を計算\n",
    "\n",
    "        loss.backward()  # ④誤差からバックプロパゲーションにより勾配を計算\n",
    "        optimizer.step()  # ⑤最適化アルゴリズムによりパラメータを更新\n",
    "\n",
    "    # 評価\n",
    "    net.eval()  # 評価モード\n",
    "    acc_train = eval(loader_train)\n",
    "    acc_test = eval(loader_test)\n",
    "    print(\"Epoch:\", epoch,\n",
    "          \"acc_train:\", str(acc_train*100) + \"%\",\n",
    "          \"acc_test:\", str(acc_test*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27kXr8NXvq4s",
    "outputId": "79d32572-bd09-4907-c27b-c5a74c4b0957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 77.08333333333334%\n"
     ]
    }
   ],
   "source": [
    "net.eval()  # 評価モード\n",
    "acc_test = eval(loader_test)\n",
    "print(\"accuracy:\", str(acc_test*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFKWDMYUv0VG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOXT7d11GmIiFjkjlmvyKFY",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
