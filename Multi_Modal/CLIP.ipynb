{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c275b72f-70e2-4a27-8593-304655e557bb",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "* \n",
    "* [hugging face](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)\n",
    "* 参考リンク  \n",
    "https://qiita.com/sonoisa/items/00e8e2861147842f0237  \n",
    "https://github.com/sonoisa/clip-japanese/blob/main/CLIP_ja_basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f48ebc-381e-4cd3-bf35-3e57a3433ca3",
   "metadata": {},
   "source": [
    "### install\n",
    "* transformers \n",
    "* torch \n",
    "* sentencepiece\n",
    "* fugashi \n",
    "* ipadic\n",
    "* !git clone https://github.com/sonoisa/clip-japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1c01d-b7fb-4e11-a468-05b9a7d31664",
   "metadata": {},
   "source": [
    "!pip install fugashi ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85820ae-1999-4512-bbb0-c6300f326278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "class ClipTextModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        super(ClipTextModel, self).__init__()\n",
    "\n",
    "        if os.path.exists(model_name_or_path):\n",
    "            # load from file system\n",
    "            output_linear_state_dict = torch.load(os.path.join(model_name_or_path, \"output_linear.bin\"), map_location=device)\n",
    "        else:\n",
    "            # download from the Hugging Face model hub\n",
    "            filename = hf_hub_download(repo_id=model_name_or_path, filename=\"output_linear.bin\")\n",
    "            output_linear_state_dict = torch.load(filename)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path)\n",
    "        config = self.model.config\n",
    "\n",
    "        self.max_cls_depth = 6\n",
    "\n",
    "        sentence_vector_size = output_linear_state_dict[\"bias\"].shape[0]\n",
    "        self.sentence_vector_size = sentence_vector_size\n",
    "        self.output_linear = nn.Linear(self.max_cls_depth * config.hidden_size, sentence_vector_size)\n",
    "        self.output_linear.load_state_dict(output_linear_state_dict)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, \n",
    "                                                       is_fast=True, do_lower_case=True)\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "    ):\n",
    "        output_states = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        token_embeddings = output_states[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        hidden_states = output_states[\"hidden_states\"]\n",
    "\n",
    "        output_vectors = []\n",
    "\n",
    "        # cls tokens\n",
    "        for i in range(1, self.max_cls_depth + 1):\n",
    "            cls_token = hidden_states[-1 * i][:, 0]\n",
    "            output_vectors.append(cls_token)\n",
    "\n",
    "        output_vector = torch.cat(output_vectors, dim=1)\n",
    "        logits = self.output_linear(output_vector)\n",
    "\n",
    "        output = (logits,) + output_states[2:]\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, texts, batch_size=8, max_length=64):\n",
    "        self.eval()\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(texts), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = texts[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(\n",
    "                batch, max_length=max_length, padding=\"longest\", \n",
    "                truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self(**encoded_input)\n",
    "            text_embeddings = model_output[0].cpu()\n",
    "\n",
    "            all_embeddings.extend(text_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)        \n",
    "\n",
    "    def save(self, output_dir):\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(self.output_linear.state_dict(), os.path.join(output_dir, \"output_linear.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8b672c-6cde-4fe5-840f-5188f9dc6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "class ClipVisionModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        super(ClipVisionModel, self).__init__()\n",
    "\n",
    "        if os.path.exists(model_name_or_path):\n",
    "            # load from file system\n",
    "            visual_projection_state_dict = torch.load(os.path.join(model_name_or_path, \"visual_projection.bin\"))\n",
    "        else:\n",
    "            # download from the Hugging Face model hub\n",
    "            filename = hf_hub_download(repo_id=model_name_or_path, filename=\"visual_projection.bin\")\n",
    "            visual_projection_state_dict = torch.load(filename)\n",
    "\n",
    "        self.model = transformers.CLIPVisionModel.from_pretrained(model_name_or_path)\n",
    "        config = self.model.config\n",
    "\n",
    "        self.feature_extractor = transformers.CLIPFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "        vision_embed_dim = config.hidden_size\n",
    "        projection_dim = 512\n",
    "\n",
    "        self.visual_projection = nn.Linear(vision_embed_dim, projection_dim, bias=False)\n",
    "        self.visual_projection.load_state_dict(visual_projection_state_dict)\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        output_states = self.model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        image_embeds = self.visual_projection(output_states[1])\n",
    "\n",
    "        return image_embeds\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_image(self, images, batch_size=8):\n",
    "        self.eval()\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(images), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = images[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.feature_extractor(batch, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self(**encoded_input)\n",
    "            image_embeddings = model_output.cpu()\n",
    "\n",
    "            all_embeddings.extend(image_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)        \n",
    "\n",
    "    @staticmethod\n",
    "    def remove_alpha_channel(image):\n",
    "        image.convert(\"RGBA\")\n",
    "        alpha = image.convert('RGBA').split()[-1]\n",
    "        background = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
    "        background.paste(image, mask=alpha)\n",
    "        image = background.convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.feature_extractor.save_pretrained(output_dir)\n",
    "        torch.save(self.visual_projection.state_dict(), os.path.join(output_dir, \"visual_projection.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f924fc-c0b3-4c76-80d0-130aacb14d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "class ClipModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        super(ClipModel, self).__init__()\n",
    "\n",
    "        if os.path.exists(model_name_or_path):\n",
    "            # load from file system\n",
    "            repo_dir = model_name_or_path\n",
    "        else:\n",
    "            # download from the Hugging Face model hub\n",
    "            repo_dir = snapshot_download(model_name_or_path)\n",
    "\n",
    "        self.text_model = ClipTextModel(repo_dir, device=device)\n",
    "        self.vision_model = ClipVisionModel(os.path.join(repo_dir, \"vision_model\"), device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit_scale = nn.Parameter(torch.ones([]) * 2.6592)\n",
    "            logit_scale.set_(torch.load(os.path.join(repo_dir, \"logit_scale.bin\"), map_location=device).clone().cpu())\n",
    "            self.logit_scale = logit_scale\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, token_type_ids):\n",
    "        image_features = self.vision_model(pixel_values=pixel_values)\n",
    "        text_features = self.text_model(input_ids=input_ids, \n",
    "                                        attention_mask=attention_mask, \n",
    "                                        token_type_ids=token_type_ids)[0]\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, images, texts, batch_size=8, max_length=64):\n",
    "        model.eval()\n",
    "        image_features = self.vision_model.encode_image(images, batch_size=batch_size)\n",
    "        text_features = self.text_model.encode_text(texts, batch_size=batch_size, max_length=max_length)\n",
    "\n",
    "        image_features = image_features.to(self.device)\n",
    "        text_features = text_features.to(self.device)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        logits_per_image = logits_per_image.cpu()\n",
    "        logits_per_text = logits_per_text.cpu()\n",
    "\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        torch.save(self.logit_scale, os.path.join(output_dir, \"logit_scale.bin\"))\n",
    "        self.text_model.save(output_dir)\n",
    "        self.vision_model.save(os.path.join(output_dir, \"vision_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2652c654-486b-44a6-944c-8833487ad7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def unicode_normalize(cls, s):\n",
    "    pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "    def norm(c):\n",
    "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "\n",
    "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "    s = re.sub('－', '-', s)\n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    s = re.sub('[ 　]+', ' ', s)\n",
    "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                      '\\u3040-\\u309F',  # HIRAGANA\n",
    "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                      ))\n",
    "    basic_latin = '\\u0000-\\u007F'\n",
    "\n",
    "    def remove_space_between(cls1, cls2, s):\n",
    "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "        while p.search(s):\n",
    "            s = p.sub(r'\\1\\2', s)\n",
    "        return s\n",
    "\n",
    "    s = remove_space_between(blocks, blocks, s)\n",
    "    s = remove_space_between(blocks, basic_latin, s)\n",
    "    s = remove_space_between(basic_latin, blocks, s)\n",
    "    return s\n",
    "\n",
    "def normalize_neologd(s):\n",
    "    s = s.strip()\n",
    "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "\n",
    "    def maketrans(f, t):\n",
    "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "\n",
    "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "    s = s.translate(\n",
    "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "    s = re.sub('[’]', '\\'', s)\n",
    "    s = re.sub('[”]', '\"', s)\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def normalize_text(text):\n",
    "    return normalize_neologd(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f84d412-3b2c-479c-a9cb-1f892346ba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925443020a0a4f4ab5fc3ace26c11cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ClipModel(\"sonoisa/clip-vit-b-32-japanese-v1\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acfc52-f6fa-4957-b7f3-31116fcb187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 類似度を求める対象の画像（16枚）\n",
    "images = [Image.open(f\"/content/clip-japanese/sample_images/{i}.jpeg\") for i in range(1, 17)]\n",
    "\n",
    "# タイリング表示\n",
    "plt.figure(dpi=140, figsize=(10,10))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    sp = plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    text = sp.text(-16, 0, f\"{i + 1}\", ha=\"right\", va=\"top\", color=\"black\", fontsize=12)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82799e23-4486-4a3a-a169-001d43c40fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 画像との類似度を求める文章\n",
    "texts = [\"猫\",\n",
    "         \"ロゼッタストーン\",\n",
    "         \"恐竜と子供\", \"恐竜\", \"子供\",  # 複数の物体が写っているとき、全体を見て類似性を判定するか？\n",
    "         \"考えるスティーブ・ジョブズの人形\",  # 考える人との混同が起きないか？\n",
    "         \"レゴでできたマリオやルイージなど\",  # レゴやマリオといった固有名詞を認識できるか？\n",
    "         \"レゴでできた時計\",  # 時計に見えるかギリギリのものを認識できるか？\n",
    "         \"魔女ランダと聖獣バロン\", \"特殊合体するとシヴァ神\",  # あまりメジャーではなさそうな存在を認識できるか？\n",
    "         \"彫刻「考える人」\",  # 考えるスティーブ・ジョブズとの混同が起きないか？\n",
    "         \"水槽の中のアンモナイト\",  # コクテンフグと見分けがつくか？\n",
    "         \"鶏とヒヨコのおもちゃ\",  # 抽象的な造形表現を認識できるか？\n",
    "         \"水槽の中の犬\", \"水槽の中のコクテンフグ\",  # 犬と錯覚するか？\n",
    "         \"お菓子が1個\", \"お菓子が2個\", \"お菓子が3個\", \"お菓子が4個\", \"お菓子が5個\",  # 数勘定できるか？\n",
    "         \"彫刻「午後の日」\", \"芸術作品\",  # 日本の芸術作品を認識できるか？\n",
    "         \"眠るコアラ\", \"木登りするコアラ\",  # 行動を識別できるか？\n",
    "         \"Apple\", \"Pineapple\",  # アルファベットを認識できるか？（英語版CLIPではできるため、できなくなっていないかの確認）\n",
    "         \"りんご\", \"パイナップル\",  # ひらがなを認識できるか？\n",
    "        ]\n",
    "\n",
    "texts = [normalize_text(text) for text in texts]  # この正規化は必須です。行わないと精度が落ちることがあります。\n",
    "\n",
    "logits_per_image, logits_per_text = model.encode(images, texts)\n",
    "\n",
    "similarity_per_image = torch.softmax(logits_per_image, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956cebe8-f86a-47c9-adbf-6afe2084d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref. https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "\n",
    "def heatmap(x_labels, y_labels, expected_answers, values):\n",
    "    fig, ax = plt.subplots(dpi=140, figsize=(8, 8))\n",
    "    im = ax.imshow(values, vmin=0, vmax=1, cmap=\"viridis\")\n",
    "\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_yticks(np.arange(len(y_labels)))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_xlabel(\"テキスト\")\n",
    "    ax.set_ylabel(\"画像\")\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\", fontsize=10,\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(len(y_labels)):\n",
    "        for j in range(len(x_labels)):\n",
    "            ax.text(j, i, \"%.2f\" % values[i, j], \n",
    "                    ha=\"center\", va=\"center\", color=\"w\", fontsize=6)\n",
    "            if expected_answers[i] == j:\n",
    "                c = patches.Circle(xy=(j, i), radius=0.5, ec='r', fill=False)\n",
    "                ax.add_patch(c)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "x_labels = texts\n",
    "y_labels = [\"1. 猫\", \"2. ロゼッタストーン\", \"3. 恐竜と子供\", \"4. ジョブズの人形\", \n",
    "            \"5. レゴでできたマリオなど\", \"6. レゴでできた時計\", \"7. 魔女ランダと聖獣バロン\", \n",
    "            \"8. 彫刻「考える人」\", \"9. アンモナイト\", \"10. 鶏とヒヨコのおもちゃ\", \"11. コクテンフグ\", \n",
    "            \"12. 和菓子\", \"13. 彫刻「午後の日」\", \"14. コアラ\", \"15. Apple\", \n",
    "            \"16. りんご\"]\n",
    "expected_answers = [0, 1, 2, 5, 6, 7, 8, 10, 11, 12, 14, 18, 20, 22, 24, 26]\n",
    "heatmap(x_labels, y_labels, expected_answers, similarity_per_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981aa0a3-2d2f-438a-8983-2ac406a6acf7",
   "metadata": {},
   "source": [
    "# 画像とテキストの埋め込み計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d972694-1f3b-499a-8188-00e31a3295a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images_3 = [Image.open(f\"/content/clip-japanese/sample_images/{i}.jpeg\") for i in range(1, 4)]\n",
    "image_features = model.vision_model.encode_image(sample_images_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b923d4-bf8f-4ffc-b7dc-a027caefc113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts_3 = [\"猫\", \"ロゼッタストーン\", \"恐竜と子供\"]\n",
    "sample_texts_3 = [normalize_text(text) for text in sample_texts_3]  # この正規化は必須です。行わないと精度が落ちることがあります。\n",
    "text_features = model.text_model.encode_text(sample_texts_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fce0b-68e5-4962-8189-7c0be01cd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単位ベクトル化\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity_matrix = image_features @ text_features.t()\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb5bb1-7575-434d-bc60-6b34fbf5c152",
   "metadata": {},
   "source": [
    "# 類似画像検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e7f10-2f56-4181-a7aa-479446d06f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索対象となる画像の埋め込みベクトルを計算しておく。\n",
    "target_images = [Image.open(f\"/content/clip-japanese/sample_images/{i}.jpeg\") for i in range(1, 17)]\n",
    "target_vectors = model.vision_model.encode_image(target_images).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29219d-cef0-4b4a-8a97-3ec29a1e941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "def search_image(query_vector, target_vectors, target_images, closest_n=3):\n",
    "    distances = cdist(\n",
    "        query_vector, target_vectors, metric=\"cosine\"\n",
    "    )[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    # タイリング表示\n",
    "    plt.figure(dpi=140, figsize=(6,6))\n",
    "\n",
    "    for i, (idx, distance) in enumerate(results[0:closest_n]):\n",
    "        image = target_images[idx]\n",
    "\n",
    "        sp = plt.subplot(math.ceil(closest_n / 4), 4, i + 1)\n",
    "        plt.imshow(image)\n",
    "        text = sp.text(-32, 0, f\"{i + 1}: {distance:0.5f}\", ha=\"left\", va=\"bottom\", color=\"black\", fontsize=12)\n",
    "        plt.axis(\"off\")    \n",
    "\n",
    "def search_image_by_text(text, target_vectors, target_images, closest_n=3):\n",
    "    text = normalize_text(text)\n",
    "    text_features = model.text_model.encode_text([text]).numpy()\n",
    "    search_image(text_features, target_vectors, target_images, closest_n)\n",
    "\n",
    "def search_image_by_image(image, target_vectors, target_images, closest_n=3):\n",
    "    image_features = model.vision_model.encode_image([image]).numpy()\n",
    "    search_image(image_features, target_vectors, target_images, closest_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb778a7-4e2c-4dc2-b3fb-ddb012067cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"猫\"\n",
    "search_image_by_text(text, target_vectors, target_images, closest_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5263c-1c22-4caa-8cf3-3f713f4951ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(f\"/content/clip-japanese/sample_images/1.jpeg\")\n",
    "search_image_by_image(image, target_vectors, target_images, closest_n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
